# Парсер хабов с habr.com

Написать парсер любого хаба с habr.com на выбор. Условия:
1) Раз в 10 минут делать запрос на главную страницу хаба.
2) Взять с главной страницы хаба ссылки на статьи.
3) Для каждой собранной ссылки посетить страницу статьи и собрать информацию о статье (заголовок, дата, ссылка на пост, имя автор, ссылка на автора).
4) Вывести информацию на консоль.

Опциональные усложнения:

✅ Сохранять данные в базу данных PostgreSQL/sqlite3 с текстом публикации (повторяющиеся по ссылкам не сохранять).

✅ Создать таблицу в базе данных с информацией о хабах. 
- Добавить в созданную таблицу 2-3 хаба.
- Обходить все хабы из таблицы, а не только один изначально выбранный.
- Сохранять для публикаций также хаб, с которого была взята публикация.

✅ Сделать парсер асинхронным, используя библиотеку aiohttp, например, 5 параллельных запросов.

❌ Добавить админку на Django для отображения хабов и управления ими (добавить хаб/удалить хаб/указать период обхода хаба).

✅ Упаковать всё в Docker образ.

Для запуска:
    
    git clone https://github.com/IndifferentD/SiteSoftTestTask3.git
    cd SiteSoftTestTask3
    docker compose up


Максимальное количество параллельных запросов и интервал парсинга задан в main.py

    PARALLEL_REQUESTS = 5
    PARSE_INTERVAL = 600


Список хабов для парсинга задан в виде сета ссылок в database/init.py

    hub_urls_to_parse = {
        'https://habr.com/ru/hubs/popular_science/articles/',
        'https://habr.com/ru/hubs/programming/articles/',
        'https://habr.com/ru/hubs/open_source/articles/',
        'https://habr.com/ru/hubs/maths/articles/'
    }

